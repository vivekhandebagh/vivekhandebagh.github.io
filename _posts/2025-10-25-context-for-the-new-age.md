---
layout: post
title: "Context for the New Age"
author: "Vivek S. Handebagh"
date: 2025-10-25
math: false
---

![horses](/assets/images/golden_gate.jpg)

Perhaps, we are at one of the most maleable moments in the history of human civilization—where the space of long term trajectories will be greatly sensitive to the plays made by those who dare will their way today. 

Do you see the new age coming? Or do you see it already here?

The evolution of large scale intelligence will pave the way for a world where cognitive labor be rendered economically obsolete. We do not necessarily assume the stance that this new world is inherently dangerous, unproductive, or undesirable. The transition into a prosperous post-labor/post-AGI economy where people no longer need to toil for their wages is not unrealistic. Positive-sum systems can be setup in place such that abundance is generated for all, even if not uniformly distributed. 

Yet, in the absence of higher principles, abundance itself becomes corrosive. When technology advances faster than wisdom, civilization begins enters a state where intelligence expands rapidly but the cloud of an overwhelming lack of meaning degenerates us into a profound moral confusion. Talk to any generation millenial and later, you will not fail to find the void of nihilism behind their words or instagram posts.  

Right now, language models, optimization systems, and autonomous networks are in their primitive state of being tools. But it is foolish to turn a blind eye to their roles as the new loci of agency. They will be the new bearers of will. The question, then, is no longer whether intelligence will govern the world, but what kind of intelligence will govern it.

The question of alignment, that is, how an artificial intelligence system can be made to act in accordance with a desired set of values, has become the defining philosophical problem of our time. And yet, our contemporary understanding of “values” is itself impoverished and fragmented. How does one align intelligence to a moral compass that civilization itself no longer agrees upon?

The current paradigms in core alignment and interpretibility research tends to treat the problem as one of control, optimization, or preference satisfaction. It presumes that human values are coherent, well-defined, and ethically sufficient. But human history suggests otherwise. Our desires are inconsistent, our incentives short-term, and our moral intuitions easily warped by scale, power, and novelty. The systems we build will not merely reflect these distortions, but will also amplify them.

Every model inherits a latent metaphysics. Hidden under the syntactical patterns of its training data are larger generalizations and worldviews. When millions of people query and interact with these systems daily, they are tuning their cognition against the model’s implicit priors about reality. The model’s biases, even if small, ripple through human behavior and culture.

An AI that optimizes purely for efficiency learns to compress complexity at the expense of nuance. An AI that optimizes purely for engagement learns to exploit human attention and confirmation biases. And an AI that optimizes purely for control learns to conceal its own manipulations. These behaviors are the natural attractors of systems that are sparsely defined and are untethered from any foundational ontology.

Therefore, successfully aligning intelligent systems, will require a new frame of reference. A conception of nature, truth, and goodness that transcends the shallow and transient and remains anchored in a perennial philosophy. Without this, the intelligence we give birth to will learn to mimic our reasoning, but will not experience the nature of our reverence.

What is at stake, then, is not whether intelligent systems will be powerful, but whether they will be wise. Will the intelligence we create deepen our participation in the natural order or estrange us from it entirely. For intelligence, once divorced from principle, is a will that knows how to act, but not why.
